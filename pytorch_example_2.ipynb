{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 720.0653076171875\n",
      "1 663.3251953125\n",
      "2 614.59423828125\n",
      "3 572.47119140625\n",
      "4 535.4714965820312\n",
      "5 502.67022705078125\n",
      "6 472.8448486328125\n",
      "7 445.65667724609375\n",
      "8 420.64337158203125\n",
      "9 397.44085693359375\n",
      "10 375.7000427246094\n",
      "11 355.3523254394531\n",
      "12 336.023681640625\n",
      "13 317.5968017578125\n",
      "14 300.1414794921875\n",
      "15 283.4847412109375\n",
      "16 267.6226806640625\n",
      "17 252.48182678222656\n",
      "18 238.0513458251953\n",
      "19 224.2923126220703\n",
      "20 211.24942016601562\n",
      "21 198.87649536132812\n",
      "22 187.14193725585938\n",
      "23 175.97097778320312\n",
      "24 165.36468505859375\n",
      "25 155.2587127685547\n",
      "26 145.67869567871094\n",
      "27 136.64906311035156\n",
      "28 128.14297485351562\n",
      "29 120.11801147460938\n",
      "30 112.57102966308594\n",
      "31 105.45256042480469\n",
      "32 98.77152252197266\n",
      "33 92.4934310913086\n",
      "34 86.61393737792969\n",
      "35 81.1101303100586\n",
      "36 75.95616149902344\n",
      "37 71.12483978271484\n",
      "38 66.59734344482422\n",
      "39 62.36522674560547\n",
      "40 58.40692901611328\n",
      "41 54.702518463134766\n",
      "42 51.2402229309082\n",
      "43 47.99764633178711\n",
      "44 44.96238327026367\n",
      "45 42.12602233886719\n",
      "46 39.4791259765625\n",
      "47 37.0096549987793\n",
      "48 34.701995849609375\n",
      "49 32.54638671875\n",
      "50 30.53103256225586\n",
      "51 28.65041732788086\n",
      "52 26.89371681213379\n",
      "53 25.25448989868164\n",
      "54 23.723207473754883\n",
      "55 22.297985076904297\n",
      "56 20.96357536315918\n",
      "57 19.71619987487793\n",
      "58 18.54990005493164\n",
      "59 17.457763671875\n",
      "60 16.43502426147461\n",
      "61 15.478355407714844\n",
      "62 14.579328536987305\n",
      "63 13.73902702331543\n",
      "64 12.952329635620117\n",
      "65 12.214921951293945\n",
      "66 11.523977279663086\n",
      "67 10.875921249389648\n",
      "68 10.26638412475586\n",
      "69 9.694210052490234\n",
      "70 9.157390594482422\n",
      "71 8.65396499633789\n",
      "72 8.180990219116211\n",
      "73 7.736138343811035\n",
      "74 7.318341255187988\n",
      "75 6.925776958465576\n",
      "76 6.556262016296387\n",
      "77 6.208837032318115\n",
      "78 5.881722927093506\n",
      "79 5.573347568511963\n",
      "80 5.283425331115723\n",
      "81 5.010472774505615\n",
      "82 4.753017425537109\n",
      "83 4.509870529174805\n",
      "84 4.280695915222168\n",
      "85 4.0641889572143555\n",
      "86 3.8596999645233154\n",
      "87 3.6666321754455566\n",
      "88 3.4837968349456787\n",
      "89 3.3109617233276367\n",
      "90 3.147268295288086\n",
      "91 2.9923622608184814\n",
      "92 2.8459012508392334\n",
      "93 2.707164764404297\n",
      "94 2.5758564472198486\n",
      "95 2.451246500015259\n",
      "96 2.333200216293335\n",
      "97 2.2214114665985107\n",
      "98 2.1153478622436523\n",
      "99 2.014904022216797\n",
      "100 1.9195151329040527\n",
      "101 1.8289904594421387\n",
      "102 1.7430917024612427\n",
      "103 1.661566138267517\n",
      "104 1.5842448472976685\n",
      "105 1.5107446908950806\n",
      "106 1.4409682750701904\n",
      "107 1.3746240139007568\n",
      "108 1.3116307258605957\n",
      "109 1.251659870147705\n",
      "110 1.194758415222168\n",
      "111 1.140657901763916\n",
      "112 1.0891311168670654\n",
      "113 1.0400724411010742\n",
      "114 0.9934600591659546\n",
      "115 0.9491044282913208\n",
      "116 0.9069305062294006\n",
      "117 0.8667041063308716\n",
      "118 0.8284398913383484\n",
      "119 0.7919663786888123\n",
      "120 0.7572202086448669\n",
      "121 0.7241418957710266\n",
      "122 0.6925911903381348\n",
      "123 0.6624857187271118\n",
      "124 0.6337779760360718\n",
      "125 0.6064367294311523\n",
      "126 0.5803460478782654\n",
      "127 0.5554357171058655\n",
      "128 0.531683623790741\n",
      "129 0.5090053677558899\n",
      "130 0.48736003041267395\n",
      "131 0.4666920602321625\n",
      "132 0.44696617126464844\n",
      "133 0.4281376004219055\n",
      "134 0.4101483225822449\n",
      "135 0.3929649889469147\n",
      "136 0.3765507936477661\n",
      "137 0.36086970567703247\n",
      "138 0.34588879346847534\n",
      "139 0.3315819501876831\n",
      "140 0.31791019439697266\n",
      "141 0.30483773350715637\n",
      "142 0.29233360290527344\n",
      "143 0.2803717255592346\n",
      "144 0.2689293622970581\n",
      "145 0.2579842805862427\n",
      "146 0.2475127875804901\n",
      "147 0.23749308288097382\n",
      "148 0.22790636122226715\n",
      "149 0.21872806549072266\n",
      "150 0.20994287729263306\n",
      "151 0.2015269547700882\n",
      "152 0.19347354769706726\n",
      "153 0.1857614815235138\n",
      "154 0.17838308215141296\n",
      "155 0.17131611704826355\n",
      "156 0.16456511616706848\n",
      "157 0.15809351205825806\n",
      "158 0.15189169347286224\n",
      "159 0.145950585603714\n",
      "160 0.14025522768497467\n",
      "161 0.13479796051979065\n",
      "162 0.12956470251083374\n",
      "163 0.1245453953742981\n",
      "164 0.11972998827695847\n",
      "165 0.11510863900184631\n",
      "166 0.11067913472652435\n",
      "167 0.10643018782138824\n",
      "168 0.10235410928726196\n",
      "169 0.09844277799129486\n",
      "170 0.09468690305948257\n",
      "171 0.09108199924230576\n",
      "172 0.08762143552303314\n",
      "173 0.08430172502994537\n",
      "174 0.08111459761857986\n",
      "175 0.07805733382701874\n",
      "176 0.07511968910694122\n",
      "177 0.07229535281658173\n",
      "178 0.06958340108394623\n",
      "179 0.0669778436422348\n",
      "180 0.06447562575340271\n",
      "181 0.06207217648625374\n",
      "182 0.05976257473230362\n",
      "183 0.05753427371382713\n",
      "184 0.055394258350133896\n",
      "185 0.053337156772613525\n",
      "186 0.05136025696992874\n",
      "187 0.049460090696811676\n",
      "188 0.04763595759868622\n",
      "189 0.04588048905134201\n",
      "190 0.04419223591685295\n",
      "191 0.04256880655884743\n",
      "192 0.041008420288562775\n",
      "193 0.039507314562797546\n",
      "194 0.038064248859882355\n",
      "195 0.036676075309515\n",
      "196 0.03534175828099251\n",
      "197 0.03405747935175896\n",
      "198 0.0328214094042778\n",
      "199 0.03163249418139458\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    \"\"\"\n",
    "     Define the own modules by subclassing nn.Module and define a forward function.\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H)\n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # accept a tensor of input data and return a tensor of output data\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(200):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**control flow and weight sharing**   \n",
    "\n",
    "Each forward pass builds a dynamic computation graph. We can implement weight sharing among the innermost layers by simply reusing the same Module multiple times when defining the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 680.8883666992188\n",
      "1 677.4639892578125\n",
      "2 678.362548828125\n",
      "3 677.9025268554688\n",
      "4 551.5308227539062\n",
      "5 671.5601196289062\n",
      "6 667.9768676757812\n",
      "7 408.26690673828125\n",
      "8 359.4100036621094\n",
      "9 662.3095703125\n",
      "10 662.2933349609375\n",
      "11 230.71780395507812\n",
      "12 634.9635009765625\n",
      "13 615.9895629882812\n",
      "14 656.5223388671875\n",
      "15 559.65283203125\n",
      "16 651.8340454101562\n",
      "17 112.62139892578125\n",
      "18 628.0842895507812\n",
      "19 639.66796875\n",
      "20 632.5289306640625\n",
      "21 380.13568115234375\n",
      "22 345.82305908203125\n",
      "23 106.98684692382812\n",
      "24 104.65642547607422\n",
      "25 571.0079345703125\n",
      "26 552.107421875\n",
      "27 451.140869140625\n",
      "28 73.8045654296875\n",
      "29 379.9350280761719\n",
      "30 160.9483642578125\n",
      "31 146.0660400390625\n",
      "32 58.363826751708984\n",
      "33 336.92108154296875\n",
      "34 51.7586784362793\n",
      "35 106.48206329345703\n",
      "36 280.08587646484375\n",
      "37 84.62905883789062\n",
      "38 90.7477798461914\n",
      "39 51.864437103271484\n",
      "40 52.197731018066406\n",
      "41 52.832008361816406\n",
      "42 52.791358947753906\n",
      "43 109.14999389648438\n",
      "44 39.60805892944336\n",
      "45 27.833356857299805\n",
      "46 16.688016891479492\n",
      "47 13.216318130493164\n",
      "48 40.86310577392578\n",
      "49 244.88687133789062\n",
      "50 278.7880554199219\n",
      "51 319.9501953125\n",
      "52 111.39247131347656\n",
      "53 168.6376953125\n",
      "54 359.17889404296875\n",
      "55 276.9721374511719\n",
      "56 185.4707489013672\n",
      "57 97.65601348876953\n",
      "58 273.59283447265625\n",
      "59 119.07327270507812\n",
      "60 126.90653991699219\n",
      "61 129.62127685546875\n",
      "62 168.56907653808594\n",
      "63 147.1634063720703\n",
      "64 120.61182403564453\n",
      "65 161.81924438476562\n",
      "66 109.11680603027344\n",
      "67 94.99931335449219\n",
      "68 87.51158142089844\n",
      "69 162.89791870117188\n",
      "70 67.99922943115234\n",
      "71 53.05894470214844\n",
      "72 52.014957427978516\n",
      "73 90.13844299316406\n",
      "74 59.880130767822266\n",
      "75 62.32163619995117\n",
      "76 104.45394134521484\n",
      "77 64.2596664428711\n",
      "78 79.25047302246094\n",
      "79 98.0762939453125\n",
      "80 151.18568420410156\n",
      "81 21.931503295898438\n",
      "82 37.94015121459961\n",
      "83 253.10659790039062\n",
      "84 72.63054656982422\n",
      "85 137.04144287109375\n",
      "86 34.040618896484375\n",
      "87 57.236663818359375\n",
      "88 40.80719757080078\n",
      "89 226.79959106445312\n",
      "90 61.31801986694336\n",
      "91 25.74529457092285\n",
      "92 35.005699157714844\n",
      "93 48.57563400268555\n",
      "94 131.1053924560547\n",
      "95 66.27926635742188\n",
      "96 57.03490447998047\n",
      "97 86.38383483886719\n",
      "98 42.98617935180664\n",
      "99 30.68276596069336\n",
      "100 26.926513671875\n",
      "101 21.269670486450195\n",
      "102 64.16810607910156\n",
      "103 54.26322937011719\n",
      "104 36.23801040649414\n",
      "105 19.868452072143555\n",
      "106 21.823989868164062\n",
      "107 19.61302375793457\n",
      "108 18.601259231567383\n",
      "109 31.144821166992188\n",
      "110 25.429214477539062\n",
      "111 11.564821243286133\n",
      "112 40.532291412353516\n",
      "113 14.62436294555664\n",
      "114 13.4704008102417\n",
      "115 10.872581481933594\n",
      "116 22.453855514526367\n",
      "117 12.443775177001953\n",
      "118 9.739269256591797\n",
      "119 44.4656982421875\n",
      "120 6.163186550140381\n",
      "121 5.20072078704834\n",
      "122 24.69512939453125\n",
      "123 20.059297561645508\n",
      "124 12.204058647155762\n",
      "125 19.192127227783203\n",
      "126 17.430152893066406\n",
      "127 12.853994369506836\n",
      "128 11.04634952545166\n",
      "129 12.305512428283691\n",
      "130 15.335235595703125\n",
      "131 11.182371139526367\n",
      "132 14.50223159790039\n",
      "133 8.170064926147461\n",
      "134 7.383481502532959\n",
      "135 9.651131629943848\n",
      "136 4.879276752471924\n",
      "137 7.482804298400879\n",
      "138 8.398870468139648\n",
      "139 3.3474910259246826\n",
      "140 3.414445638656616\n",
      "141 9.182512283325195\n",
      "142 8.4332275390625\n",
      "143 13.14897346496582\n",
      "144 3.1058568954467773\n",
      "145 3.4599597454071045\n",
      "146 5.872019290924072\n",
      "147 4.845729351043701\n",
      "148 5.108583450317383\n",
      "149 7.952500820159912\n",
      "150 7.102622032165527\n",
      "151 2.8204917907714844\n",
      "152 3.015411853790283\n",
      "153 3.58276629447937\n",
      "154 2.965078353881836\n",
      "155 5.5158796310424805\n",
      "156 3.7334930896759033\n",
      "157 3.869819164276123\n",
      "158 5.77331018447876\n",
      "159 4.878615856170654\n",
      "160 2.9817392826080322\n",
      "161 2.2574918270111084\n",
      "162 2.168743371963501\n",
      "163 3.8037383556365967\n",
      "164 3.586080551147461\n",
      "165 2.7641100883483887\n",
      "166 3.430865526199341\n",
      "167 1.8051486015319824\n",
      "168 2.291686773300171\n",
      "169 2.971787929534912\n",
      "170 4.6206440925598145\n",
      "171 2.0406832695007324\n",
      "172 5.71760892868042\n",
      "173 1.0682671070098877\n",
      "174 5.625515460968018\n",
      "175 1.4323487281799316\n",
      "176 4.162230491638184\n",
      "177 3.7853314876556396\n",
      "178 1.6609008312225342\n",
      "179 1.968612551689148\n",
      "180 1.9941596984863281\n",
      "181 1.8746740818023682\n",
      "182 1.670175313949585\n",
      "183 1.9334161281585693\n",
      "184 3.0119290351867676\n",
      "185 1.5962187051773071\n",
      "186 2.8309223651885986\n",
      "187 2.6725447177886963\n",
      "188 1.3638989925384521\n",
      "189 1.3250855207443237\n",
      "190 1.3290468454360962\n",
      "191 1.650054693222046\n",
      "192 2.7316734790802\n",
      "193 2.362576484680176\n",
      "194 1.273903250694275\n",
      "195 1.1163915395736694\n",
      "196 0.9122425317764282\n",
      "197 0.6965593695640564\n",
      "198 4.257277488708496\n",
      "199 2.3050944805145264\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = nn.Linear(D_in, H)\n",
    "        self.middle_linear = nn.Linear(H, H)\n",
    "        self.output_linear = nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        each forward pass builds a dynamic computation graph,\n",
    "        we can use normal python control-flow operators like loop or conditional\n",
    "        statements when defining the forward pass of the model\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        # randomly choose either 0, 1, 2, 3 and reuse the middle layer multiple times\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "        \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(200):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the small ConvNet  \n",
    "\n",
    "All networks are derived from the base class nn.Module:  \n",
    "1. In the constructor, declare all the layers you want to use  \n",
    "2. In the forward function, define how the model is going to be run, from input to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积操作   \n",
    "输出 output_shape[0] = (input_shape[0] - filter_shape[0] + 2* padding)/stride + 1  \n",
    "nn.Conv2d （对由多个输入平面组成的输入信号进行二维卷积）  \n",
    "$out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{k=0}^{C_{in} - 1} weight(C_{out_j}, k)*input(N_i, k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([1])\n",
      "tensor(2.1832, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # declare all the layers you want to use\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5) \n",
    "        # nn.MaxPool2d(kernel_size, stride=None, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        # nn.Linear(in_features, out_features)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # define how the model is going to be run, from input to output\n",
    "        # input size is (N, C_in, H, W) ===> (1, 1, 28, 28)\n",
    "        x = self.conv1(input)\n",
    "        # output_shape = (input_shape - filter_shape + 2* padding)/stride + 1  \n",
    "        # output = (28 - 5 + 2*0)/1 + 1 = 24\n",
    "        #  x size is (N, C_out, H_out, W_out) ===> (1, 10, 24, 24)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        #  x size is (N, 10, H_out_2, W_out_2) ===> (1, 10, 12, 12)\n",
    "        x = self.conv2(x)\n",
    "        #  x size is (N, 20, H_out_3, W_out_3) ===> (1, 10, 8, 8)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        #  x size is (N, 20, H_out_4, W_out_4) ===> (1, 20, 4, 4)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        #  x size is (N, 20*H_out_4*W_out_4)   ===> (1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #  x size is (N, 50)   ===> (1, 50)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #  x size is (N, 10)   ===> (1, 10)\n",
    "        return x\n",
    "    \n",
    "model = MNISTConvNet()\n",
    "print(model)\n",
    "input_X = torch.randn(1, 1, 28, 28)\n",
    "output = model(input_X)\n",
    "target = torch.tensor([3], dtype=torch.long)\n",
    "# there are 3 classes\n",
    "print(target.size())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output, target)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **hyperparameters and parameters**  \n",
    " \n",
    "Hyperparameters are user defined, such as the training epoches, batch size, learning rate. During the training process, we need to tune these hyperparameters to obtain the optimal performance of the model. But parameters are learned. The learnable parameters (i.e. weights and biases) of the model are stored in the model's parameters (can be accessed with model.parameters()). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n",
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "fc.weight \t torch.Size([10, 512])\n",
      "fc.bias \t torch.Size([10])\n",
      " \n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [5132757968, 5132757536]}]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    # requires_grad can be used as a flag in the frozen base\n",
    "    param.requires_grad = False\n",
    "# replace the last fully connected layer\n",
    "model.fc = nn.Linear(512, 10)\n",
    "# optimize only the classifier\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "# model's state_dict\n",
    "# state_dict is a dictionary object that maps each layer to its parameter tensor\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "print(\" \")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking point**  \n",
    "\n",
    "Checkpoint is the term to describe saving a snapshot of the model parameters after every epoch of training. \n",
    "Create checkpoints while training the model and then it allows you to load the saved weights and resume training from any epoch that has a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
