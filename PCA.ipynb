{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**:  \n",
    "主成分分析从原始数据中的特征中重新构建了新的特征，剔除了特征之间存在的冗余的信息。让我们用少量的新的特征就可以最大程度的重构出原始数据所包含的信息。新的协方差矩阵的对角线上的方差是新维度上的方差，非对角线表示的维度之间的相关性。x，y之间的相关程度越小，就是噪声越小，信息的冗余程度越小。新的特征拥有更好的区分度。  \n",
    "\n",
    "It is a method of summarizing some data. Each object in the dataset has a list of different characteristics. But many of them will measure related properties and will be redudant. Besides, the computational cost increases rapidly. If so, we should able to summarize each object with fewer characteristics. That could help us to reduce the high dimensionality to low dimensionality.  Criterion for good representation (with no classification information) is to minimize information loss.\n",
    "\n",
    "PCA is not just selecting some characteristics and discarding the others. Instead, it constructs some new characteristics using the old one. In fact, PCA finds the best possible characteristics. The properties (characteristics) that strongly differ across objects. That means objects are different but if the new constructed feature makes them all look the same, it would certainly be a bad summary. in this way, the property shows as much variation across objects as possible. The higher the variance, the lower the error. So the maximum variance and the minimum error are reached at the same time.\n",
    "\n",
    "As the square symmetric matrix (here is the covariance matrix) can be diagonalized by choosing a new orthogonal coordinate system. Corresponding eigenvalues will be located on the diagnoal. The correlation between the points is zero that means there is no relationship with each other, it reduces the redundant information. The direction of the first components is given by the first eigenvector of the covariance matrix.  \n",
    "\n",
    "covariance matrix X eigenvectors = eigenvalue X egienvectors  \n",
    "\n",
    "1. The K-th largest eigenvalue of covariance matric is the variance of the K-th Principal Component.   \n",
    "2. The K-th Principal Component retains the K-th largest variation in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedures:   \n",
    "求去中心化的协方差矩阵，协方差矩阵对角化，求到特征值，特征向量，再得到PCs\n",
    "Let **X** be of n x p size, where n is the number of samples and p is the number of features. Assumed it is centered, i.e. columns means have been substracted and are equal to zero.   \n",
    "\n",
    "\n",
    "Then **C** is the covariance matrix of p x p size, $$C = \\frac{X^T X}{n-1}$$  \n",
    "\n",
    "It is a symmetric matrix so it can be diagonalized $$ C = VLV^T$$  \n",
    "where V is a matrix of eigenvectors ( each column is the eigenvector ) and L is a diagonal matrix with eigenvalues $\\lambda_i$ in the decreasing order on the diagnoal.   \n",
    "\n",
    "The eigenvectors are called principal axes or principal directions of the data. Projections of the data on the principal axes are called principal components, also known as PC scores. The j-th principal component is given by the j-th column of **XV**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedures:  \n",
    "如果我们使用svd分解的方法\n",
    "**X** is centered, we perform Singular Value Decomposition (SVD) then we obtain\n",
    "$$ X = USV^T $$  \n",
    "The columns of V are principal direction/axes.  \n",
    "Singular values are related to the eigenvalues of covariance matrix via $$ \\lambda_i = \\frac{s_i^2}{n-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]]) # 3 samples, 2 features\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "Mean = np.mean(A, axis = 0)\n",
    "A_centered = A - Mean\n",
    "print(A_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4.],\n",
       "       [4., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# covariance matrix (2x2)\n",
    "C = np.cov(A_centered.T)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8. 0.]\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# eigen decomposition \n",
    "values, vectors = np.linalg.eig(C)\n",
    "print(values)\n",
    "print(vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.82842712,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 2.82842712,  0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# project data\n",
    "Pc = np.dot(A_centered, vectors)\n",
    "Pc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
