{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the network has a single hidden layer, and trained with gradient descent to fit random data by minimizing the Euclidean Distance between the network output and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36654028.18116052\n",
      "1 28197456.956982538\n",
      "2 21242109.045911424\n",
      "3 14862627.06859262\n",
      "4 9696387.940155469\n",
      "5 6168767.99634973\n",
      "6 3996093.0025573918\n",
      "7 2712226.499879354\n",
      "8 1946455.300414504\n",
      "9 1468762.988361637\n",
      "10 1152297.2499678887\n",
      "11 929597.06630546\n",
      "12 764841.2601159154\n",
      "13 638092.6214065547\n",
      "14 537756.4153417265\n",
      "15 456636.9612356018\n",
      "16 390347.29433500156\n",
      "17 335593.69841093617\n",
      "18 289912.57062667276\n",
      "19 251524.06264928984\n",
      "20 219067.6696289307\n",
      "21 191450.75940984854\n",
      "22 167862.2932737589\n",
      "23 147623.07571460304\n",
      "24 130216.47740580156\n",
      "25 115167.00796112863\n",
      "26 102116.00817778298\n",
      "27 90757.17807441804\n",
      "28 80853.53070319138\n",
      "29 72191.13804025375\n",
      "30 64588.80390164518\n",
      "31 57900.91213407631\n",
      "32 52002.14056246617\n",
      "33 46796.04190707661\n",
      "34 42183.909421806966\n",
      "35 38090.59925078027\n",
      "36 34451.524199374515\n",
      "37 31206.811799337796\n",
      "38 28308.869315687156\n",
      "39 25716.69515785575\n",
      "40 23393.598653487734\n",
      "41 21308.064769368808\n",
      "42 19432.543454765182\n",
      "43 17743.95130174278\n",
      "44 16220.471900958995\n",
      "45 14844.914862997764\n",
      "46 13600.30254150804\n",
      "47 12473.09046067273\n",
      "48 11450.899666388006\n",
      "49 10523.280655306366\n",
      "50 9679.76387404001\n",
      "51 8912.085418842516\n",
      "52 8212.447630538078\n",
      "53 7574.279429967456\n",
      "54 6991.517159373545\n",
      "55 6459.162327348204\n",
      "56 5971.9218383261505\n",
      "57 5525.816211530608\n",
      "58 5117.306704123479\n",
      "59 4742.382633935921\n",
      "60 4397.979786477973\n",
      "61 4081.5176161288637\n",
      "62 3790.657142793325\n",
      "63 3523.209593462856\n",
      "64 3276.521846833898\n",
      "65 3049.0497885321884\n",
      "66 2839.0209416819584\n",
      "67 2645.0893732681398\n",
      "68 2465.7415073180314\n",
      "69 2299.808720239262\n",
      "70 2146.153753586001\n",
      "71 2003.8262763172834\n",
      "72 1871.9145480287386\n",
      "73 1749.5486184405545\n",
      "74 1635.922458304009\n",
      "75 1530.3821083286216\n",
      "76 1432.3310645710887\n",
      "77 1341.2110423310583\n",
      "78 1256.3844525947866\n",
      "79 1177.4339519171047\n",
      "80 1103.8876908811776\n",
      "81 1035.3435898505031\n",
      "82 971.4272141675532\n",
      "83 911.8188970821651\n",
      "84 856.1987188182563\n",
      "85 804.2780232712332\n",
      "86 755.7770910428615\n",
      "87 710.4378117239995\n",
      "88 668.0488149943394\n",
      "89 628.3923034203104\n",
      "90 591.2734306157984\n",
      "91 556.5257369058662\n",
      "92 524.0003527306144\n",
      "93 493.5042106144017\n",
      "94 464.92284588115655\n",
      "95 438.1259213731397\n",
      "96 412.98810651292814\n",
      "97 389.3990790856409\n",
      "98 367.2646088098146\n",
      "99 346.47558062463406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "W1 = np.random.randn(D_in, H)\n",
    "W2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(100):\n",
    "    h = x.dot(W1) # (N, H)\n",
    "    h_relu = np.maximum(h, 0) \n",
    "    y_pred = h_relu.dot(W2) # (N, D_out)\n",
    "    \n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    # backprop \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(W2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    W1 -= learning_rate * grad_w1\n",
    "    W2 -= learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27176834.0\n",
      "1 22766068.0\n",
      "2 22151140.0\n",
      "3 22036676.0\n",
      "4 20487846.0\n",
      "5 16815894.0\n",
      "6 12086071.0\n",
      "7 7769292.0\n",
      "8 4718732.5\n",
      "9 2862527.5\n",
      "10 1821410.125\n",
      "11 1244342.125\n",
      "12 915261.625\n",
      "13 715256.8125\n",
      "14 584199.4375\n",
      "15 491610.78125\n",
      "16 421862.5\n",
      "17 366613.9375\n",
      "18 321355.5625\n",
      "19 283530.78125\n",
      "20 251369.46875\n",
      "21 223739.109375\n",
      "22 199822.421875\n",
      "23 178986.296875\n",
      "24 160740.3125\n",
      "25 144700.796875\n",
      "26 130561.3203125\n",
      "27 118053.6328125\n",
      "28 106941.953125\n",
      "29 97050.0\n",
      "30 88243.6640625\n",
      "31 80361.703125\n",
      "32 73288.0\n",
      "33 66927.5\n",
      "34 61195.32421875\n",
      "35 56018.28125\n",
      "36 51332.8984375\n",
      "37 47087.55078125\n",
      "38 43231.4765625\n",
      "39 39726.3359375\n",
      "40 36535.33203125\n",
      "41 33627.921875\n",
      "42 30976.568359375\n",
      "43 28554.404296875\n",
      "44 26339.52734375\n",
      "45 24311.759765625\n",
      "46 22453.88671875\n",
      "47 20749.75390625\n",
      "48 19186.29296875\n",
      "49 17751.40625\n",
      "50 16433.798828125\n",
      "51 15221.5791015625\n",
      "52 14105.7265625\n",
      "53 13077.990234375\n",
      "54 12130.48828125\n",
      "55 11256.6923828125\n",
      "56 10450.2744140625\n",
      "57 9705.708984375\n",
      "58 9018.15625\n",
      "59 8383.12890625\n",
      "60 7796.921875\n",
      "61 7255.9384765625\n",
      "62 6755.22265625\n",
      "63 6291.396484375\n",
      "64 5864.40771484375\n",
      "65 5468.6416015625\n",
      "66 5101.39501953125\n",
      "67 4760.53515625\n",
      "68 4443.955078125\n",
      "69 4149.96728515625\n",
      "70 3876.6806640625\n",
      "71 3622.574462890625\n",
      "72 3386.247802734375\n",
      "73 3166.37548828125\n",
      "74 2961.65087890625\n",
      "75 2771.02587890625\n",
      "76 2593.50244140625\n",
      "77 2428.091796875\n",
      "78 2273.87158203125\n",
      "79 2130.07666015625\n",
      "80 1995.937744140625\n",
      "81 1870.7694091796875\n",
      "82 1753.9654541015625\n",
      "83 1644.8582763671875\n",
      "84 1542.99853515625\n",
      "85 1447.816162109375\n",
      "86 1358.852783203125\n",
      "87 1275.6905517578125\n",
      "88 1197.9451904296875\n",
      "89 1125.207275390625\n",
      "90 1057.17041015625\n",
      "91 993.474853515625\n",
      "92 933.8735961914062\n",
      "93 878.0472412109375\n",
      "94 825.7616577148438\n",
      "95 776.759765625\n",
      "96 730.8543701171875\n",
      "97 687.810302734375\n",
      "98 647.4447631835938\n",
      "99 609.5931396484375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in).to(device)\n",
    "y = torch.randn(N, D_out).to(device)\n",
    "\n",
    "w1 = torch.randn(D_in, H).to(device)\n",
    "w2 = torch.randn(H, D_out).to(device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in range(100):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(i, loss.item())\n",
    "    # Backprop\n",
    "    grad_y_pred = 2.0*(y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone().detach()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    # update\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***backward will compute the gradient of loss with respect to all Tensors with require_grad = True***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33453980.0\n",
      "1 33688708.0\n",
      "2 37670824.0\n",
      "3 38762856.0\n",
      "4 32185628.0\n",
      "5 20498954.0\n",
      "6 10425639.0\n",
      "7 4907647.5\n",
      "8 2518488.0\n",
      "9 1536416.375\n",
      "10 1094438.0\n",
      "11 857644.625\n",
      "12 705739.6875\n",
      "13 595057.1875\n",
      "14 508419.0625\n",
      "15 437985.34375\n",
      "16 379593.375\n",
      "17 330573.75\n",
      "18 289043.4375\n",
      "19 253662.265625\n",
      "20 223333.484375\n",
      "21 197217.1875\n",
      "22 174631.234375\n",
      "23 155011.453125\n",
      "24 137912.078125\n",
      "25 123012.71875\n",
      "26 109941.1328125\n",
      "27 98446.140625\n",
      "28 88316.546875\n",
      "29 79361.453125\n",
      "30 71423.984375\n",
      "31 64380.64453125\n",
      "32 58116.16015625\n",
      "33 52528.0625\n",
      "34 47535.1953125\n",
      "35 43068.3203125\n",
      "36 39065.4765625\n",
      "37 35471.5703125\n",
      "38 32240.47265625\n",
      "39 29338.5\n",
      "40 26726.376953125\n",
      "41 24368.49609375\n",
      "42 22238.58203125\n",
      "43 20311.6015625\n",
      "44 18567.23828125\n",
      "45 16985.8046875\n",
      "46 15550.9306640625\n",
      "47 14248.48046875\n",
      "48 13064.96875\n",
      "49 11988.431640625\n",
      "50 11008.490234375\n",
      "51 10115.294921875\n",
      "52 9300.5810546875\n",
      "53 8556.375\n",
      "54 7876.25244140625\n",
      "55 7254.37646484375\n",
      "56 6685.3623046875\n",
      "57 6164.33154296875\n",
      "58 5686.62939453125\n",
      "59 5248.68798828125\n",
      "60 4846.77783203125\n",
      "61 4478.0126953125\n",
      "62 4139.06689453125\n",
      "63 3827.165771484375\n",
      "64 3540.343994140625\n",
      "65 3276.33056640625\n",
      "66 3033.3798828125\n",
      "67 2809.53564453125\n",
      "68 2603.23828125\n",
      "69 2413.10546875\n",
      "70 2237.598876953125\n",
      "71 2075.73974609375\n",
      "72 1926.224609375\n",
      "73 1788.16943359375\n",
      "74 1660.5748291015625\n",
      "75 1542.6158447265625\n",
      "76 1433.527099609375\n",
      "77 1332.5865478515625\n",
      "78 1239.1263427734375\n",
      "79 1152.589599609375\n",
      "80 1072.42529296875\n",
      "81 998.1102905273438\n",
      "82 929.2578125\n",
      "83 865.3624877929688\n",
      "84 806.1575317382812\n",
      "85 751.2320556640625\n",
      "86 700.2371826171875\n",
      "87 652.8694458007812\n",
      "88 608.8709716796875\n",
      "89 567.959716796875\n",
      "90 529.9321899414062\n",
      "91 494.58074951171875\n",
      "92 461.6878967285156\n",
      "93 431.07806396484375\n",
      "94 402.5572204589844\n",
      "95 375.99761962890625\n",
      "96 351.2742004394531\n",
      "97 328.24798583984375\n",
      "98 306.78802490234375\n",
      "99 286.7872314453125\n"
     ]
    }
   ],
   "source": [
    "# pytorch autograd\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w1.requires_grad_()\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "w2.requires_grad_()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in range(100):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(i, loss.item())\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # we don't want to build up a computational graph for the update steps\n",
    "    # to signify that you donâ€™t want to track the gradient of this operation \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # manually zero the gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The forward function computes output Tensors from input Tensors, the backward function receives the gradient of the output Tensors with respect to some scalar value***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 692.7584838867188\n",
      "1 639.4838256835938\n",
      "2 593.3998413085938\n",
      "3 553.3582763671875\n",
      "4 518.0360717773438\n",
      "5 486.5527038574219\n",
      "6 458.27740478515625\n",
      "7 432.5351257324219\n",
      "8 408.91461181640625\n",
      "9 387.0406494140625\n",
      "10 366.61871337890625\n",
      "11 347.3252258300781\n",
      "12 329.1490478515625\n",
      "13 312.07244873046875\n",
      "14 295.83270263671875\n",
      "15 280.4412536621094\n",
      "16 265.8695373535156\n",
      "17 251.9599151611328\n",
      "18 238.63462829589844\n",
      "19 225.9536895751953\n",
      "20 213.90664672851562\n",
      "21 202.43356323242188\n",
      "22 191.53472900390625\n",
      "23 181.18177795410156\n",
      "24 171.32241821289062\n",
      "25 161.94676208496094\n",
      "26 153.02313232421875\n",
      "27 144.55917358398438\n",
      "28 136.52207946777344\n",
      "29 128.8887176513672\n",
      "30 121.6481704711914\n",
      "31 114.76158905029297\n",
      "32 108.23185729980469\n",
      "33 102.06263732910156\n",
      "34 96.24482727050781\n",
      "35 90.75540161132812\n",
      "36 85.57328033447266\n",
      "37 80.68270874023438\n",
      "38 76.06634521484375\n",
      "39 71.7142562866211\n",
      "40 67.61351013183594\n",
      "41 63.74716567993164\n",
      "42 60.11194610595703\n",
      "43 56.69011688232422\n",
      "44 53.45947265625\n",
      "45 50.41518020629883\n",
      "46 47.54682922363281\n",
      "47 44.85174560546875\n",
      "48 42.3128547668457\n",
      "49 39.91659927368164\n",
      "50 37.65840148925781\n",
      "51 35.53520965576172\n",
      "52 33.5344352722168\n",
      "53 31.65403175354004\n",
      "54 29.88875961303711\n",
      "55 28.228649139404297\n",
      "56 26.669567108154297\n",
      "57 25.20403480529785\n",
      "58 23.824417114257812\n",
      "59 22.525623321533203\n",
      "60 21.302581787109375\n",
      "61 20.151092529296875\n",
      "62 19.065433502197266\n",
      "63 18.041683197021484\n",
      "64 17.077404022216797\n",
      "65 16.17022132873535\n",
      "66 15.314592361450195\n",
      "67 14.506741523742676\n",
      "68 13.745431900024414\n",
      "69 13.02840805053711\n",
      "70 12.352928161621094\n",
      "71 11.714849472045898\n",
      "72 11.112236976623535\n",
      "73 10.542917251586914\n",
      "74 10.003802299499512\n",
      "75 9.49390697479248\n",
      "76 9.012380599975586\n",
      "77 8.556396484375\n",
      "78 8.125370025634766\n",
      "79 7.717575550079346\n",
      "80 7.3318328857421875\n",
      "81 6.966609001159668\n",
      "82 6.620698928833008\n",
      "83 6.2933807373046875\n",
      "84 5.983381748199463\n",
      "85 5.689964294433594\n",
      "86 5.4117865562438965\n",
      "87 5.148183822631836\n",
      "88 4.898527145385742\n",
      "89 4.661560535430908\n",
      "90 4.436956405639648\n",
      "91 4.222985744476318\n",
      "92 4.0199127197265625\n",
      "93 3.8273813724517822\n",
      "94 3.644465923309326\n",
      "95 3.4708244800567627\n",
      "96 3.306007146835327\n",
      "97 3.149552345275879\n",
      "98 3.0008678436279297\n",
      "99 2.8594725131988525\n"
     ]
    }
   ],
   "source": [
    "# arrange the computations into layer, some of learnable parameters can be optimized\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# use nn package to define our model as a sequence of layers\n",
    "# apply them in sequence to produce its output\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "for i in range(100):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(i, loss.item())\n",
    "    \n",
    "    # zero the gradients before running the model\n",
    "    model.zero_grad()\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # compute gradients for all learnable parameters in the model\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***optimization algorithms***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 634.1229248046875\n",
      "1 617.5852661132812\n",
      "2 601.5380859375\n",
      "3 586.152099609375\n",
      "4 571.2894897460938\n",
      "5 556.8876342773438\n",
      "6 542.943603515625\n",
      "7 529.4150390625\n",
      "8 516.3544921875\n",
      "9 503.6574401855469\n",
      "10 491.3299255371094\n",
      "11 479.3149108886719\n",
      "12 467.62542724609375\n",
      "13 456.3007507324219\n",
      "14 445.3126220703125\n",
      "15 434.6336669921875\n",
      "16 424.3106384277344\n",
      "17 414.3401794433594\n",
      "18 404.6278381347656\n",
      "19 395.12872314453125\n",
      "20 385.8719482421875\n",
      "21 376.87567138671875\n",
      "22 368.1364440917969\n",
      "23 359.66925048828125\n",
      "24 351.43389892578125\n",
      "25 343.4422912597656\n",
      "26 335.66522216796875\n",
      "27 328.06268310546875\n",
      "28 320.65167236328125\n",
      "29 313.38177490234375\n",
      "30 306.3014831542969\n",
      "31 299.384765625\n",
      "32 292.6275634765625\n",
      "33 285.9807434082031\n",
      "34 279.45489501953125\n",
      "35 273.0585632324219\n",
      "36 266.7910461425781\n",
      "37 260.6462097167969\n",
      "38 254.59970092773438\n",
      "39 248.69773864746094\n",
      "40 242.93630981445312\n",
      "41 237.30020141601562\n",
      "42 231.76422119140625\n",
      "43 226.33609008789062\n",
      "44 221.00799560546875\n",
      "45 215.77293395996094\n",
      "46 210.63722229003906\n",
      "47 205.59471130371094\n",
      "48 200.6380157470703\n",
      "49 195.7816925048828\n",
      "50 191.01210021972656\n",
      "51 186.33642578125\n",
      "52 181.7490234375\n",
      "53 177.26589965820312\n",
      "54 172.86981201171875\n",
      "55 168.56854248046875\n",
      "56 164.35498046875\n",
      "57 160.23202514648438\n",
      "58 156.1904296875\n",
      "59 152.24732971191406\n",
      "60 148.38485717773438\n",
      "61 144.59246826171875\n",
      "62 140.8797607421875\n",
      "63 137.2314453125\n",
      "64 133.6549530029297\n",
      "65 130.153076171875\n",
      "66 126.72346496582031\n",
      "67 123.36444091796875\n",
      "68 120.07913208007812\n",
      "69 116.87660217285156\n",
      "70 113.73934936523438\n",
      "71 110.66648864746094\n",
      "72 107.66429138183594\n",
      "73 104.7267074584961\n",
      "74 101.85398864746094\n",
      "75 99.04622650146484\n",
      "76 96.29769897460938\n",
      "77 93.60999298095703\n",
      "78 90.98472595214844\n",
      "79 88.42179870605469\n",
      "80 85.91996765136719\n",
      "81 83.4715347290039\n",
      "82 81.07750701904297\n",
      "83 78.74126434326172\n",
      "84 76.45934295654297\n",
      "85 74.23284912109375\n",
      "86 72.0582275390625\n",
      "87 69.93938446044922\n",
      "88 67.87059020996094\n",
      "89 65.84941101074219\n",
      "90 63.8779296875\n",
      "91 61.956520080566406\n",
      "92 60.08454895019531\n",
      "93 58.25553894042969\n",
      "94 56.47916793823242\n",
      "95 54.74938201904297\n",
      "96 53.06105422973633\n",
      "97 51.417903900146484\n",
      "98 49.81694793701172\n",
      "99 48.25634002685547\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# optimizer will update the weights of the model \n",
    "# the first param to the optimiezer tells which tensor it should update\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(100):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # makes an update to its parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
