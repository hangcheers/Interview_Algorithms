{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习中，有两大方法即理论优美，又在实践中取得很好效果。一是SVM及其衍生的Kernel，二是AdaBoost及其衍生的 Boosting\n",
    "\n",
    "### 线性二分类模型\n",
    "\n",
    "Data = ${(x_1, y_1), (x_2, y_2), ... (x_n, y_n)}$ , $x_i \\in R^d, y \\in {-1, 1}$,   \n",
    "而分类的目的是，学得一个假设函数 $h: R \\rightarrow {-1, 1}$  \n",
    "if $y_i = 1, h(x_i)=1$   \n",
    "if $y_i = -1, h(x_i)=-1$  \n",
    "$\\forall i,  y_i h(x_i) = 1$  \n",
    "更进一步是认为假设函数的形式是基于$x_i$的线性组合，$h(x_i) := sign(w^T x_i + b)$   \n",
    "因此线性二分类的目标是**找到一组合适参数**$(w, b)$ 使得 $\\forall i, y_i(w^T x_i + b) > 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "SVM 涉及到 perceptron， kernel trick 和 statistical learning theory 三个部分的知识。  \n",
    "\n",
    "\n",
    "SVMs take the training set to start with and try to predict the labels for the test set. \n",
    "Take SVM for linearly seperable binary classification as an example.\n",
    "Find some lines that can divide the input space into two seperate regions. The learning rule can be applied to find the correct data. If the data point is on the right side, the leearning rule does nothing but if the data point is on the wrong side, it should adjust the line. \n",
    "The best choice is the hyperplane that leave the maximum margin from both classes. (这样的话，hyperplane 离各类样本最远，模型比较稳健，不容易因为样本的随机扰动而使得样本到达hyperplane的另一侧，而产生分类错误)  \n",
    "\n",
    "The hyperplane can be expressed as:\n",
    "$$ w^T x + b = 0$$  \n",
    "The distance of the point to the hyperplane is:  \n",
    "$$ proj_w (p-x) = \\frac{|w^T p + b| }{||w||} $$  \n",
    "The goal can be expressed as:\n",
    "$$ max_{w, b} \\space min_{i}\\frac{2}{||w||}|w^T x_i + b| $$\n",
    "$$ s.t. y_i(w^T x_i + b) > 0, i = 1, 2, ..., m$$\n",
    "Once we get the optimal hyperplane, that is the classifier. Then find out the which side of the line the test point lies.  \n",
    "\n",
    "To solve the problems that are not linearly seperable, the input space can be projected into another feature space. In this space, the data may become linearly separable. (E.g the blue and red point are not the linearly seperable, we use a polynomial kernel that can be linearly seperable in the 3-D space) ( 看视频 https://www.youtube.com/watch?v=3liCbRZPrZA  )    \n",
    "\n",
    "Different SVM Kernel functions (kernel的选择影响了整体的性能):  \n",
    "1. Linear Kernel \n",
    "2. Polynomial Kernel\n",
    "$$ k(x_i, x_j) = (x_i \\cdot x_j + 1)^d $$\n",
    "3. Gaussian RBF Kernel ( general-purpose kernel, used when there is no prior knowledge about the data)\n",
    "$$ k(x_i, x_j) = exp(-\\gamma ||x_i - x_j||^2)$$  \n",
    "4. Gaussian Kernel  \n",
    "$$ k(x, y) = exp(-\\frac{-||x-y||^2}{2\\sigma^2})$$\n",
    "当feature的维度超过了样本数的时候，使用线性kernel;当feature的维度较小，样本的数量中等时，使用rbf kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
